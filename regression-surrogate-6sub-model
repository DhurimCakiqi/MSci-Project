#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Oct 26 17:34:49 2020

@author: haogao
"""

import time
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#%matplotlib inline 
import os
from parameters import parameters as pm
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor

import warnings
warnings.filterwarnings('ignore')


df=pd.read_excel(pm.file_datasets,index_col=0)


df.columns

df.columns=['q1', 'q2', 'q3', 'q4', r'$\alpha^v$', r'$\beta^v$', r'$\alpha^1$',
       r'$\beta^1$', r'$\alpha^2$', r'$\beta^2$']

df.describe()

df.head(5)


feature_columns=[ r'$\alpha^v$', r'$\beta^v$', r'$\alpha^1$',
       r'$\beta^1$', r'$\alpha^2$', r'$\beta^2$']
for col in feature_columns:
    df[col]=(df[col]-df[col].mean())/df[col].std()
    
df[feature_columns].boxplot()
plt.gca().set_ylabel("value",fontdict={"size":18})
plt.gca().set_xlabel("feature name",fontdict={'family' : 'Times New Roman', 'size' : 18})
plt.show()    

def box_plot_outliers(data_ser, box_scale):
    """
    利用箱线图去除异常值
    :param data_ser: 接收 pandas.Series 数据格式
    :param box_scale: 箱线图尺度，
    :return:
    """
    iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))
    val_low = data_ser.quantile(0.25) - iqr
    val_up = data_ser.quantile(0.75) + iqr
    low_index = data_ser[data_ser < val_low].index
    up_index = data_ser[data_ser > val_up].index

    return list(low_index) + list(up_index)


error_index = []
for col in feature_columns:
    error_index += box_plot_outliers(df[col],box_scale=3)
df=df.drop(error_index)


q_data,y_data=df.iloc[:,:4],df.iloc[:,4:]
train_size=int(0.9*len(q_data))
q_data_train,q_data_test=q_data.iloc[:train_size,:],q_data.iloc[train_size:,:]
y_data_train,y_data_test=y_data.iloc[:train_size,:],y_data.iloc[train_size:,:]


q_data_exp,y_data_exp=df.iloc[-1,:4].values.reshape((1,-1)),df.iloc[-1,4:].values.reshape((1,-1))
q_data_train.shape,q_data_test.shape,q_data_exp.shape,y_data_exp.shape


# Class to extend the Sklearn regressor
class SklearnHelper(object):
    def __init__(self, clf, params=None,isMuilt_reg=False):
        if isMuilt_reg: self.clf= MultiOutputRegressor (clf(**params))
        else: self.clf = clf(**params)

    def train(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)
    
    def score(self,x,y):
        return self.clf.score(x,y)
    
    def feature_importances(self,x,y):
        print(self.clf.fit(x,y).feature_importances_)
        
        
def mse_loss(y_true,y_pred):
    return np.mean(np.square(y_true-y_pred))

X = q_data_train.values
y = y_data_train.values
X_test = q_data_test.values
y_test=y_data_test.values        


X.shape,y.shape,X_test.shape,y_test.shape


'''
'n_neighbors': 1, 2, 4, 6, 8, 10
'p': 1, 2


'''
knn_para=[
    {'n_neighbors': 12, 'p': 1,
     'algorithm': 'brute', 'weights': 'distance', 'metric': 'minkowski', 'n_jobs': -1},
    {'n_neighbors': 8, 'p': 1,
     'algorithm': 'brute', 'weights': 'distance', 'metric': 'minkowski', 'n_jobs': -1},
    {'n_neighbors': 10, 'p': 2,
     'algorithm': 'brute', 'weights': 'distance', 'metric': 'minkowski', 'n_jobs': -1},
    {'n_neighbors': 6, 'p': 1,
     'algorithm': 'brute', 'weights': 'distance', 'metric': 'minkowski', 'n_jobs': -1},
    {'n_neighbors': 6, 'p': 2,
     'algorithm': 'brute', 'weights': 'distance', 'metric': 'minkowski', 'n_jobs': -1},
    {'n_neighbors': 8, 'p': 1,
     'algorithm': 'brute', 'weights': 'distance', 'metric': 'minkowski', 'n_jobs': -1},
]
feat_flag = 5
sub = KNeighborsRegressor(**knn_para[feat_flag])
sub.fit(X, y[:, feat_flag])
pred = sub.predict(X_test)
print('Test: mse loss={:.6f} r2_score={:.6f}'.format(
            mse_loss(y_test[:, feat_flag], pred),metrics.r2_score(y_test[:, feat_flag], pred)))



'''
'n_estimators':100,50,75,150
'max_depth':4,6,8,16
'learning_rate':0.05,0.1,0.15
'''
xgb_para=[
    {'n_estimators': 100, 'max_depth': 8, 'learning_rate':0.1,
    'objective':'reg:squarederror', 'booster':'gbtree', 'n_jobs':-1},
    {'n_estimators': 100, 'max_depth': 8, 'learning_rate':0.1,
    'objective':'reg:squarederror', 'booster':'gbtree', 'n_jobs':-1},
    {'n_estimators': 100, 'max_depth': 8, 'learning_rate':0.1,
    'objective':'reg:squarederror', 'booster':'gbtree', 'n_jobs':-1},
    {'n_estimators': 75, 'max_depth': 8, 'learning_rate':0.1,
    'objective':'reg:squarederror', 'booster':'gbtree', 'n_jobs':-1},
    {'n_estimators': 100, 'max_depth': 8, 'learning_rate':0.1,
    'objective':'reg:squarederror', 'booster':'gbtree', 'n_jobs':-1},
    {'n_estimators': 75, 'max_depth': 6, 'learning_rate':0.1,
    'objective':'reg:squarederror', 'booster':'gbtree', 'n_jobs':-1},
]
feat_flag = 5
sub = XGBRegressor(**xgb_para[feat_flag])
sub.fit(X, y[:, feat_flag])
pred = sub.predict(X_test)
print('Test: mse loss={:.6f} r2_score={:.6f}'.format(
            mse_loss(y_test[:, feat_flag], pred),metrics.r2_score(y_test[:, feat_flag], pred)))


grid_params ={
    'hidden_layer_sizes' : [(256,), (512,), (1024,)],
    'learning_rate_init' : [5e-3, 1e-2, 5e-2, 0.05],
    'activation' : ['relu'],
    'early_stopping' : [True],
    'tol' : [1e-4],
    'n_iter_no_change' : [10],
    'max_iter' : [200]
}
clf = GridSearchCV(MLPRegressor(), grid_params, scoring = 'r2', n_jobs = -1, cv = 5)

feat_flag = 3
clf.fit(X, y[:, feat_flag])

clf.best_params_


'''
'hidden_layer_sizes': (50,),(100,),(150,)
'learning_rate_init':5e-3,1e-2,1e-3,5e-2
'''
mlp_para = [
            {'hidden_layer_sizes' : (75,), 'learning_rate_init' : 5e-3, 'activation' : 'relu',
                            'early_stopping' : True, 'tol' : 1e-4, 'n_iter_no_change' : 10, 'max_iter' : 200},
            {'hidden_layer_sizes' : (75,), 'learning_rate_init' : 5e-3, 'activation' : 'relu',
                            'early_stopping' : True, 'tol' : 1e-4, 'n_iter_no_change' : 10, 'max_iter' : 200},
            {'hidden_layer_sizes' : (100,), 'learning_rate_init' : 5e-3, 'activation' : 'relu',
                            'early_stopping' : True, 'tol' : 1e-4, 'n_iter_no_change' : 10, 'max_iter' : 200},
            {'hidden_layer_sizes' : (200,), 'learning_rate_init' : 0.001, 'activation' : 'relu',
                            'early_stopping' : True, 'tol' : 1e-4, 'n_iter_no_change' : 10, 'max_iter' : 200,
                            'random_state' : 42},
            {'hidden_layer_sizes' : (100,), 'learning_rate_init' : 5e-3, 'activation' : 'relu',
                            'early_stopping' : True, 'tol' : 1e-4, 'n_iter_no_change' : 10, 'max_iter' : 200},
            {'hidden_layer_sizes' : (75,), 'learning_rate_init' : 5e-3, 'activation' : 'relu',
                            'early_stopping' : True, 'tol' : 1e-4, 'n_iter_no_change' : 10, 'max_iter' : 200},
        ]


feat_flag = 3
sub = MLPRegressor(**mlp_para[feat_flag])
sub.fit(X, y[:, feat_flag])
pred = sub.predict(X_test)
print('Test: mse loss={:.6f} r2_score={:.6f}'.format(
            mse_loss(y_test[:, feat_flag], pred),metrics.r2_score(y_test[:, feat_flag], pred)))



fold = KFold(n_splits=5, shuffle=True, random_state=42)


def run_6sub_oof(clf, params):
    models = []
    pred = np.zeros(y_test.shape)
    oof = np.zeros(y.shape)
    for index, (train_idx, val_idx) in enumerate(fold.split(X)):
        x_tr, y_tr = X[train_idx,:], y[train_idx, :]
        x_val, y_val = X[val_idx,:], y[val_idx, :]

        tr_pred = np.zeros(y_tr.shape)
        val_pred = np.zeros(y_val.shape)
        test_pred = np.zeros(y_test.shape)
        for ix, para in enumerate (params):
            sub_model = clf(**para)
            sub_model.fit(x_tr, y_tr[:, ix])
            tr_pred[:, ix] = sub_model.predict(x_tr)
            val_pred[:, ix] = sub_model.predict(x_val)
            test_pred[:, ix] = sub_model.predict(X_test)
            models.append(sub_model) 
        oof[val_idx, :] = val_pred
        print('_'*100)
        print(index+1, 'Train: mse loss = {:.6f} r2_score = {:.6f}, Validation: mse loss = {:.6f} r2_score = {:.6f}'.format(
            mse_loss(y_tr, tr_pred), metrics.r2_score(y_tr, tr_pred),
            mse_loss(y_val, val_pred), metrics.r2_score(y_val, val_pred)))
        pred = pred+ test_pred/fold.n_splits
        del x_tr, y_tr, x_val, y_val
    print("#"*100)
    print('Test: mse loss={:.6f} r2_score={:.6f}'.format(
            mse_loss(y, oof),metrics.r2_score(y_test, pred)))
    return models,oof,pred


knn_models, knn_oof, knn_pred = run_6sub_oof(KNeighborsRegressor, knn_para)

xgb_models, xgb_oof, xgb_pred = run_6sub_oof(XGBRegressor, xgb_para)

mlp_models, mlp_oof, mlp_pred = run_6sub_oof(MLPRegressor, mlp_para)



print('Test: mse loss={:.6f} r2 score={}'.format(mse_loss(y_test,xgb_pred),metrics.r2_score(y_test,xgb_pred)))

from sklearn.externals import joblib
joblib.dump(knn_models, "./Results/knn_model.m")

models = joblib.load("./Results/knn_model.m")










